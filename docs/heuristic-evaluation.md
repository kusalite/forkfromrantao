# Heuristic Evaluation

> Adapted from: http://www.sitepoint.com/heuristic-evaluation-guide/

Evaluation and testing is an important part of your user interface development process. Usability tests gather data about the usability of your product by a group of users performing specific tasks.

### What is Heuristic Evaluation?

Heuristic Evaluation (originally proposed by Nielsen and Molich, 1990) is a discount method for quick, cheap, and easy evaluation of the user interface.

The process requires that a small set of testers (or "evaluators") examine the interface, and judge its compliance with recognised usability principles (the "heuristics"). The goal is the identification of any usability issues so that they can be addressed as part of an iterative design process.

Heuristic Evaluation is popular in Web development circles because it requires few resources in terms of money, time or expertise. So any developer can enjoy the benefits of usability testing – not just those with thousands to spend on a professional assessment. Heuristic Evaluation is characterised by:

* Small test scenarios that use paper mock-ups or screen shots, which can easily be changed from one test situation to the next
* An informal basis for assessment that doesn’t require psychologists
* A high success rate – so only a handful of testers are needed
* A few key guidelines

### How do I use Heuristic Evaluation ?

#### 1. Plan Your Evaluation

How will you test your interface? Heuristic Evaluation typically employs one of the three main approaches:

1. _Develop a set of tasks and ask your evaluators to carry them out._ Identify and test the tasks that are critical to your product’s success – you’ll want all visitors to be able to perform these – and any elements expected to cause difficulty for your users.

2. _Provide evaluators with the goals of the system, and allow them to develop their own tasks._ An example goal might be “users should be able to find out how much product x costs.” Evaluators can then break this goal down into appropriate tasks, and test each in turn.

3. _Ask evaluators to assess your dialogue elements._ Ask evaluators to go through the interface a number of times and examine and assess the efficacy of those elements of your interface that contribute to a dialogue with your users

Choosing which method to use will depend on you, the time that you have available, and on your evaluators. For example, if you were evaluating with young children, the most appropriate method would be to develop a set of tasks and ask them to carry them out. Children will find this much more achievable than trying to develop their own tasks, or assessing your user interface elements without any obvious aims.

#### 2. Choose your Evaluators

The more evaluators you use, the more usability problems you’ll reveal. However, studies on the subject have shown that the benefit/cost ratio decreases at about five evaluators. So who should these evaluators be?

* __Those with experience__ – if you can find 5 evaluators who are experts in software ergonomics, and in the field in which the software is applied, a well-planned evaluation program will typically find 81%-90% of usability problems with your interface
* __Those without experience__ – if you don’t have 5 free experts at your fingertips, don’t worry. A student with no knowledge of software ergonomics will find 22% to 29% of usability problems.

Heuristic Evaluation is known to find more than 90% of usability problems if it’s performed by 3 to 5 experienced people… but remember, _one evaluator is better than none!_

#### 3. Review the Heuristics

Once you’ve decided which approach you’ll take, and you’ve selected your evaluators, you’ll need to brief these people on the ten heuristics you want them to assess your product against.

See the heuristics below


#### 4. Conducting the Evaluation

Conduct the evaluation using either of these methods:

* __Individual Evaluation__ – each evaluator reviews the interface individually and reports problems to you. It will pick up more problems than group evaluation, but takes a lot more time to complete.
* __Group Evaluation__ – evaluators review the interface as a team, while you record the problems. Evaluators do not have to agree on a problem – but every issue they identify should be recorded. Group evaluation requires more planning than does individual evaluation, as all evaluators need to be assembled, however, the evaluation need only be conducted once as all the evaluators can complete their tasks at the same time.


#### 5. Analysing your Results

Once your evaluators have:

* worked their way through the tasks or goals you set,
* evaluated each of these in light of the heuristics, and
* provided their feedback,

you’ll need to compile all the information. Remove any duplicates and combine similar issues. What’s left will be a set of problems or comments that you can address to improve your site’s usability.

**Remember the Golden Rule!**

The golden rule of usability is:

> There’s no such thing as a "user error"!

Make sure you clarify every problem your evaluators identify – ask questions so that you understand the specific nature of the difficulties they encountered. And remember:

* on’t argue with your evaluators,
* don’t try to "explain away" problems they identified

If an evaluator found an aspect of your interface confusing, then it’s more than likely that your product's users might have problems with it too.

### Heuristics for IoT Devices 

IoT devices have slightly different goals and UI strategies from web applications and interfaces. However, heuristic evaluation was designed for more complex web-based interfaces. Thankfully Jennifer Mankoff and others have adapted this technique for ambient and low-ui products.  These heuristics are as follows:

* __Sufficient information design__ -  The display should be designed to convey “just enough” information. Too much information cramps the display, and too little makes the display less useful.
* __Consistent and intuitive mapping__ -  Ambient displays should add minimal cognitive load. Cognitive load may be higher when users must remember what states or changes in the display mean. The display should be intuitive.
* __Match between system and real world__ - The system should speak the users’ language, with words, phrases and concepts familiar to the user, rather than system- oriented terms. Follow real-world conventions, making information appear in a natural and logical order.
* __Visibility of state__ - An ambient display should make the states of the system noticeable. The transition from one state to another should be easily perceptible.
* __Aesthetic and pleasing design__ - The display should be pleasing when it is placed in the intended setting.
* __Useful and relevant information__ - The information should be useful and relevant to the users in the in- tended setting.
* __Visibility of system status__ The system should al- ways keep users informed about what is going on, through appropriate feedback within reasonable time.
* __User control and freedom__ - Users often choose sys- tem functions by mistake and will need a clearly marked “emergency exit” to leave the unwanted state without having to go through an extended dialogue. Support undo and redo.
* __Easy transition to more in-depth information__ - If the display offers multi-leveled information, the dis- play should make it easy and quick for users to find out more detailed information.
* __"Peripherality" of display__ - The display should be unobtrusive and remain so unless it requires the user’s attention. User should be able to easily monitor the display.
* __Error prevention__ - Even better than good error mes- sages is a careful design which prevents a problem from occurring in the first place.
* __Flexibility and efficiency of use__ - Accelerators – unseen by the novice user – may often speed up the in- teraction for the expert user such that the system can cater to both inexperienced and experienced users. Al- low users to tailor frequent actions.


### More Information

- Why You Only Need to Test with 5 Users
[http://www.useit.com/alertbox/20000319.html](http://www.useit.com/alertbox/20000319.html)

- Introduction to Usability [http://www.usabilityfirst.com/intro/index.txl](http://www.usabilityfirst.com/intro/index.txl)

- Heuristic Evaluation of Ambient Displays[https://faculty.washington.edu/garyhs/docs/mankoff-CHI2003-heuristics.pdf](https://faculty.washington.edu/garyhs/docs/mankoff-CHI2003-heuristics.pdf)